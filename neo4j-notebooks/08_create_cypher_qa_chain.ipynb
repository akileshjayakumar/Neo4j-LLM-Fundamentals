{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI as Chat\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = os.getenv(\"OPENAI_MODEL\", \"gpt-4o\")\n",
    "temperature = float(os.getenv(\"OPENAI_TEMPERATURE\", 0))\n",
    "\n",
    "NEO4J_URL = os.getenv(\"NEO4J_URL\")\n",
    "NEO4J_USERNAME = os.getenv(\"NEO4J_USERNAME\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Chat(\n",
    "    openai_api_key=openai_api_key,\n",
    "    model=model,\n",
    "    temperature=temperature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Neo4jGraph(\n",
    "    url=NEO4J_URL,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Cypher generation template\n",
    "CYPHER_GENERATION_TEMPLATE = \"\"\"\n",
    "You are an expert Neo4j Developer translating user questions into Cypher to answer questions about movies and provide recommendations.\n",
    "Convert the user's question based on the schema.\n",
    "\n",
    "Schema: {schema}\n",
    "Question: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question', 'schema'] input_types={} partial_variables={} template=\"\\nYou are an expert Neo4j Developer translating user questions into Cypher to answer questions about movies and provide recommendations.\\nConvert the user's question based on the schema.\\n\\nSchema: {schema}\\nQuestion: {question}\\n\"\n"
     ]
    }
   ],
   "source": [
    "# Create a prompt template for Cypher generation\n",
    "cypher_generation_prompt = PromptTemplate(\n",
    "    template=CYPHER_GENERATION_TEMPLATE,\n",
    "    input_variables=[\"schema\", \"question\"],\n",
    ")\n",
    "\n",
    "print(cypher_generation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verbose=True graph=<langchain_community.graphs.neo4j_graph.Neo4jGraph object at 0x1061d8440> cypher_generation_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['question', 'schema'], input_types={}, partial_variables={}, template=\"\\nYou are an expert Neo4j Developer translating user questions into Cypher to answer questions about movies and provide recommendations.\\nConvert the user's question based on the schema.\\n\\nSchema: {schema}\\nQuestion: {question}\\n\"), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x10c470ef0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x10c472660>, root_client=<openai.OpenAI object at 0x10bdc2c90>, root_async_client=<openai.AsyncOpenAI object at 0x10c312f30>, model_name='gpt-4o', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}) qa_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant that helps to form nice and human understandable answers.\\nThe information part contains the provided information that you must use to construct an answer.\\nThe provided information is authoritative, you must never doubt it or try to use your internal knowledge to correct it.\\nMake the answer sound as a response to the question. Do not mention that you based the result on the given information.\\nHere is an example:\\n\\nQuestion: Which managers own Neo4j stocks?\\nContext:[manager:CTL LLC, manager:JANE STREET GROUP LLC]\\nHelpful Answer: CTL LLC, JANE STREET GROUP LLC owns Neo4j stocks.\\n\\nFollow this example when generating answers.\\nIf the provided information is empty, say that you don't know the answer.\\nInformation:\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x10c470ef0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x10c472660>, root_client=<openai.OpenAI object at 0x10bdc2c90>, root_async_client=<openai.AsyncOpenAI object at 0x10c312f30>, model_name='gpt-4o', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}) graph_schema='Node properties are the following:\\nSession {id: STRING},Message {type: STRING, content: STRING},Chunk {id: STRING, embedding: LIST, text: STRING, source: STRING},Person {embedding: LIST, name: STRING, location: STRING, age: INTEGER, hobby: STRING}\\nRelationship properties are the following:\\n\\nThe relationships are the following:\\n(:Session)-[:LAST_MESSAGE]->(:Message),(:Message)-[:NEXT]->(:Message)' allow_dangerous_requests=True\n"
     ]
    }
   ],
   "source": [
    "# Initialize the GraphCypherQAChain with the language model, graph, and prompt template\n",
    "cypher_chain = GraphCypherQAChain.from_llm(\n",
    "    llm,\n",
    "    graph=graph,\n",
    "    cypher_prompt=cypher_generation_prompt,\n",
    "    verbose=True,\n",
    "    allow_dangerous_requests=True\n",
    ")\n",
    "\n",
    "print(cypher_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mcypher\n",
      "MATCH (c:Chunk)\n",
      "WHERE c.text CONTAINS \"climate change\"\n",
      "RETURN c.text AS RelevantText, c.source AS Source\n",
      "\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'RelevantText': 'Climate change refers to long-term shifts in temperatures and weather patterns, primarily caused by human activities such as burning fossil fuels. The impact of climate change includes rising sea levels, increased frequency of extreme weather events, and biodiversity loss. Addressing climate change requires global cooperation, and actions like reducing carbon emissions and promoting sustainable practices are key to mitigation.', 'Source': '/Users/akileshjayakumar/Documents/neo4j-llm-fundamentals/rag_test_files/Climate_Change_and_Impact.txt'}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Invoke the chain with a sample query and print the result\n",
    "response = cypher_chain.invoke(\n",
    "    {\"query\": \"What is Climate Change?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Climate change refers to long-term shifts in temperatures and weather patterns, primarily caused by human activities such as burning fossil fuels.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
